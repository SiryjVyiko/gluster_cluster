type: update                                                                                                                                                                                                                    
version: 1.5                                                                                                                                                                                                                     
id: gluster_cluster                                                                                                                                                                                                              
baseUrl: https://raw.githubusercontent.com/SiryjVyiko/gluster_cluster/master 
description: 
  short: GlusterFS Cluster Replication Logic

name: GlusterFS Cluster Replication Logic

permanent: true

globals:
  replicatedPath: ${settings.replicatedPath:/data}
                                                                                                                                                                              
onInstall:        
  initiateGlusterFSCluster

onAfterStart:
  - cmd[storage]: mount -a; exportfs -ra;

onAfterServiceScaleOut[storage]:                                                                                                                                                                                                 
  - forEach(newnode:event.response.nodes):                                                                                                                                                                                       
      - addBrickToVolume:                                                                                                                                                                                                          
          address: ${@newnode.address}                                                                                                                                                                                             
          id: ${@newnode.id}
                                                                                                                                                                                                                                 
onBeforeScaleIn[storage]:                                                                                                                                             
  - forEach(removednode:event.response.nodes):                                                                                                                                                                                   
      removeBrickFromVolume:                                                                                                                                                                                                     
        address: ${@removednode.address}     
                                                                                                                                                                                                                                 
onAfterRedeployContainer[storage]:                                                                                                                                                                                                    
  - cmd[storage]: /bin/systemctl start glusterd.service && /bin/systemctl enable glusterd.service;
  - cmd[${nodes.storage.master.id}]: set -o pipefail; gluster --log-level=TRACE --log-file=/dev/stdout volume start data force | tee -a /var/log/glusterfs/glusterd.log;
  - cmd[${nodes.storage.master.id}]: gluster volume set data network.ping-timeout 3;
  - forEach(clusternode:nodes.storage):                                                                                                                                                                                          
      - mountDirectoryToVolume:                                                                                                                                                                                                    
          id: ${@clusternode.id}                                                                                                                                                                                                   
          address: ${@clusternode.address}                                                                                                                                                                                   
      - addAutoMount:                                                                                                                                                                                                    
          id: ${@clusternode.id}                                                                                                                                                                                                   
          address: ${@clusternode.address}
      
onAfterClone:
  if ('${fn.compareEngine(6.1.1):}' < 0):
    reinstallGlusterFSLogic:
      clonedEnv: ${event.response.env.envName}

onAfterCloneAllNodes:
  reinstallGlusterFSLogic:
    clonedEnv: ${event.params.targetenv}
      
onBeforeMigrate:
  - cmd[storage]: systemctl stop $(systemd-escape -p --suffix=automount ${globals.replicatedPath}); umount -l ${globals.replicatedPath} || true; sed -i '/glusterfs/d' /etc/fstab; rm -rf ${globals.replicatedPath}
  
onAfterMigrate:
  initiateGlusterFSCluster

actions:
  initiateGlusterFSCluster:
    - cmd[storage]: systemctl stop $(systemd-escape -p --suffix=automount ${globals.replicatedPath}); umount ${globals.replicatedPath} || true;
    - environment.file.read: 
        nodeId: ${nodes.storage.master.id}
        path: /etc/exports
        user: root
    - environment.file.write:
        nodeGroup: storage
        path: /etc/exports
        user: root
        body: ${response.body}
    - forEach(clusternode:nodes.storage):                                                                                                                                                                                          
        cleanupNode:                                                                                                                                                                                                           
          id: ${@clusternode.id}     
    - forEach(clusternode:nodes.storage):                                                                                                                                                                                          
        enableGlusterFS:                                                                                                                                                                                                           
          id: ${@clusternode.id}                                                                                                                                                                                                   
    - prepareVolumeBricks                                                                                                                                                                                                          
    - initiateVolume                                                                                                                                                                                                             
    - forEach(clusternode:nodes.storage):                                                                                                                                                                                          
        - mountDirectoryToVolume:                                                                                                                                                                                                    
            id: ${@clusternode.id}                                                                                                                                                                                                   
            address: ${@clusternode.address}                                                                                                                                                                                      
        - addAutoMount:                                                                                                                                                                                                    
            id: ${@clusternode.id}                                                                                                                                                                                                   
            address: ${@clusternode.address}
        - fixExistingMounts:                                                                                                                                                                                                    
            id: ${@clusternode.id}  
        - checkStatus:                                                                                                                                                                                                    
            id: ${@clusternode.id}  
        
  cleanupNode:                                                                                                                                                                                                              
    - cmd[${this.id}]: |-
        service glusterd stop; GLUSTER_PROCESS=$(ps aux|grep gluster|grep -v grep|awk '{print $2}'); 
        [ -n "${GLUSTER_PROCESS}" ] && kill -9 ${GLUSTER_PROCESS}; sed -i '/glusterfs/d' /etc/fstab; rm -rf /var/lib/glusterd/{vols,ss_brick,peers};
      user: root
      
  enableGlusterFS:                                                                                                                                                                                                              
    - cmd[${this.id}]: |-
        /bin/systemctl enable glusterd.service; /bin/systemctl start glusterd.service; mkdir -p /glustervolume ${globals.replicatedPath}; 
        echo -e "/glustervolume\n/var/lib/glusterd/\n/var/log/glusterfs\n${globals.replicatedPath}\n/etc/exports" >> /etc/jelastic/redeploy.conf;
        sed -i '/^$/d' /etc/exports;
      user: root
  
  addPeer:
    - cmd[${nodes.storage.master.id}]: set -o pipefail; gluster --log-level=TRACE --log-file=/dev/stdout peer probe ${this.address} | tee -a /var/log/glusterfs/glusterd.log && sleep 1;
                                                                                                                                                                                                                                 
  prepareVolumeBricks:                                                                                                                                                                                                                                                                                                                                                                                                 
    forEach(clusternode:nodes.storage):                                                                                                                                                                                          
      if (${@clusternode.id} != ${nodes.storage.master.id}):                                                                                                                                                                     
        addPeer:
            address: ${@clusternode.address}
          
  initiateVolume:
    - cmd[${nodes.storage.master.id}]: |-
        let "NUMBER_OF_BRICKS=$(gluster peer status |grep 'Number of Peers'|awk '{print $4}') + 1"; 
        BRICKS_ADDRESSES=$(gluster peer status|grep Hostname| awk '{print $2}'); BRICKS_STRING="${nodes.storage.master.address}:/glustervolume"; 
        for i in ${BRICKS_ADDRESSES}; do BRICKS_STRING="${BRICKS_STRING} ${i}:/glustervolume"; done; 
        gluster volume create data replica ${NUMBER_OF_BRICKS} transport tcp ${BRICKS_STRING} force; gluster volume start data; gluster volume set data network.ping-timeout 3; 
        [ -f /etc/jelastic/data/clients ] || gluster volume set data auth.allow 127.0.0.1;
      user: root
      
  mountDirectoryToVolume:
    - cmd[${this.id}]: |-
        dataBackupDir=$(mktemp -d)
        [ -n "$(ls -A /${globals.replicatedPath})" ] && { shopt -s dotglob; mv ${globals.replicatedPath}/* ${dataBackupDir}/; shopt -u dotglob; };
        mount.glusterfs localhost:/data ${globals.replicatedPath}
        chmod 777 ${globals.replicatedPath}
        [ -n "$(ls -A /${dataBackupDir})" ] && { shopt -s dotglob; mv ${dataBackupDir}/* ${globals.replicatedPath}/; shopt -u dotglob; };
        rm -rf ${dataBackupDir}
      user: root
      
  addNewNodeToVolume:
    - cmd[${nodes.storage.master.id}]: |-
        let "NUMBER_OF_BRICKS=$(gluster peer status |grep 'Number of Peers'|awk '{print $4}') + 1"; 
        gluster volume add-brick data replica ${NUMBER_OF_BRICKS} ${this.address}:/glustervolume force
      user: root

  addAutoMount:
    - cmd[${this.id}]: |-
        sed -i '/glusterfs/d' /etc/fstab;
        echo "localhost:/data  ${globals.replicatedPath}   glusterfs   defaults,_netdev,x-systemd.automount 0 0" >> /etc/fstab;
        
  addBrickToVolume:
    - enableGlusterFS:
        id: ${this.id}
    - addPeer:
        address: ${this.address}
    - addNewNodeToVolume:
        address: ${this.address}
    - mountDirectoryToVolume:
        id: ${this.id}
        address: ${this.address}
    - addAutoMount:
        id: ${this.id}
        address: ${this.address}
    - fixExistingMounts: 
        id: ${this.id}
        
  fixExistingMounts:
    - cmd[${this.id}]: |-
        LINE_NUMBER=0
        while read LINE; do let LINE_NUMBER++; FSID=$(cat /proc/sys/kernel/random/uuid); grep -q '^\"${globals.replicatedPath}' <<< $LINE && sed -i "${LINE_NUMBER}s/)$/,fsid=${FSID})/" /etc/exports || true; done < /etc/exports;
        exportfs -ra;
  checkStatus:
    - cmd[${this.id}]: |-
        gluster volume status;
        
  removeBrickFromVolume:
    - cmd[${nodes.storage.master.id}]: |-
        NUMBER_OF_BRICKS=$(gluster peer status |grep 'Number of Peers'|awk '{print $4}'); 
        yes 2>/dev/null | gluster volume remove-brick data replica ${NUMBER_OF_BRICKS} ${this.address}:/glustervolume force; 
        gluster peer detach ${this.address} && sleep 1;

  reinstallGlusterFSLogic:
    install:
      jps: ${baseUrl}/replication-logic-JE-57563.jps?_r=${fn.random}
      envName: ${this.clonedEnv}
      nodeGroup: storage
      settings:
        replicatedPath: {globals.replicatedPath}
